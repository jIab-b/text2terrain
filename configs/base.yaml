# Base configuration for Text2Terrain training

# Model configuration
model:
  name: "mistralai/Mistral-7B-Instruct-v0.1"
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  hidden_dropout: 0.1

# Training configuration
training:
  epochs: 3
  batch_size: 16
  learning_rate: 5.0e-4
  weight_decay: 0.01
  warmup_steps: 100
  
  # Loss weights
  module_loss_weight: 1.0
  param_loss_weight: 5.0
  
  # Evaluation
  eval_every: 1
  save_every: 1

# Data configuration
data:
  tile_size: 256
  max_sequence_length: 128
  val_split: 0.1
  num_workers: 4

# Optimization
optimizer:
  type: "adamw"
  lr: 5.0e-4
  weight_decay: 0.01
  eps: 1.0e-8
  betas: [0.9, 0.999]

# Scheduler (optional)
scheduler:
  type: "linear"
  warmup_steps: 100

# Logging
logging:
  wandb_project: "text2terrain"
  log_every: 10
  save_top_k: 3

# Hardware
hardware:
  precision: "fp16"  # or "fp32", "bf16"
  compile: false     # PyTorch 2.0 compile
  device: "auto"     # "auto", "cpu", "cuda"

# Paths (will be overridden by CLI arguments)
paths:
  data_path: "data/processed"
  output_path: "models/base"
  checkpoint_path: null